{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b359c2",
   "metadata": {},
   "source": [
    "# Wolf Sighting Prediction - Data Preperation\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "The preprocessing pipeline transforms raw wolf sightings data into a structured, enriched dataset suitable for modeling. It includes the following steps:\n",
    "\n",
    "1. **Data Loading**: Sightings data is retrieved from a remote database\n",
    "2. **Geocoding**: Place names are matched to geographic coordinates using fuzzy string matching\n",
    "3. **Spatial Binning and Clustering** Coordinates are discretized into spatial bins and KMeans clustering is applied to group sightings into regions\n",
    "4. **Temporal Feature Engineering**: The month and season are extracted from the timestamp of sightings\n",
    "5. **Historical Context Features**: The number of recent sightings and the time since the last sighting are calculated per feature\n",
    "\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "All the necessary libraries are imported here. They are listed in `requirements.txt` and can be installed using the following command:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import json\n",
    "from rapidfuzz import process\n",
    "import folium\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "from dbrepo.RestClient import RestClient\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86e11a",
   "metadata": {},
   "source": [
    "### Load Data from the Database Repository\n",
    "\n",
    "First, we need to fetch the raw sightings data from the database repository, hosted at dbrepo.tuwien.ac.at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33823609",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_widget = DownloadDataWidget(\n",
    "    use_auth=False,\n",
    "    database_id=\"4c9ac630-7ec5-491c-b727-0bea3224da91\", # id of the database\n",
    "    table_id=\"e0105e9b-acb6-4844-b3bf-aa59f35bf056\", # id of the \"Raw Sightings Data\" table\n",
    ")\n",
    "load_data_widget.display()\n",
    "load_data_widget.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92ccd5",
   "metadata": {},
   "source": [
    "### Spatial Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1b22f",
   "metadata": {},
   "source": [
    "#### Geocoding\n",
    "\n",
    "Since the raw sightings data does not contain any geospatial information, we need to geocode the place names to obtain their latitude and longitude. A file containing all towns and villages in Upper Austria (extracted from OpenStreetMap data) is provided in the `data` folder. We load this file, and attempt to match the place names in the sightings data with the town names in the provided file. This is done using the `rapidfuzz` library, which allows for approximate string matching. If a close enough match is found, we store the coordinates with the respective sighting entry. Otherwise, we remove the entry from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54edc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeonameDictionary:\n",
    "    def __init__(self, file_path):\n",
    "        self.geoname_dict = {}\n",
    "        self.place_names = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                    name = entry.get(\"name\")\n",
    "                    lat = entry.get(\"lat\")\n",
    "                    lon = entry.get(\"lon\")\n",
    "                    if name and lat is not None and lon is not None:\n",
    "                        self.geoname_dict[name] = (lat, lon)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # Skip lines that aren't valid JSON\n",
    "            self.place_names = list(self.geoname_dict.keys())\n",
    "\n",
    "    # Function to get closest match\n",
    "    def lookup(self, query):\n",
    "        match, score, _ = process.extractOne(query, self.place_names)\n",
    "        return match, score\n",
    "\n",
    "    # Function to get coordinates\n",
    "    def get_coordinates(self, place_name):\n",
    "        if place_name in self.geoname_dict:\n",
    "            return self.geoname_dict[place_name]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "geoname_dictionary = GeonameDictionary(geonames_path)\n",
    "\n",
    "processed_df = load_data_widget.get_data().copy()\n",
    "\n",
    "# Geocode each location in the DataFrame\n",
    "for index, row in processed_df.iterrows():\n",
    "    location = row.get(\"ort\")\n",
    "    if not location:\n",
    "        processed_df.drop(index, inplace=True)\n",
    "        continue\n",
    "    match, score = geoname_dictionary.lookup(location)\n",
    "    if score > 80:  # Adjust threshold as needed\n",
    "        lat, lon = geoname_dictionary.get_coordinates(match)\n",
    "        processed_df.at[index, 'lat'] = lat\n",
    "        processed_df.at[index, 'lon'] = lon\n",
    "        if score < 100:\n",
    "            print(f\"{location} -> {match} ({score})\")\n",
    "    else:\n",
    "        # If no match found, remove data entry\n",
    "        print(f\"No match found for {location}, dropping entry.\")\n",
    "        processed_df.drop(index, inplace=True)\n",
    "\n",
    "print(\"Processed DataFrame with geocoded locations\")\n",
    "print(f\"Out of {len(load_data_widget.get_data())} entries, {len(processed_df)} were kept.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3af02",
   "metadata": {},
   "source": [
    "#### Binning\n",
    "\n",
    "We create coordinate bins that can be used as categorical features by the model. This is done by snapping the coordinates to a grid, where each cell covers 0.33° in latitude and 0.25° in longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eea597",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['coord_bin'] = processed_df.apply(lambda row: coordinate_bin(row['lat'], row['lon']), axis=1).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163dc24",
   "metadata": {},
   "source": [
    "#### Clustering\n",
    "\n",
    "Next, the sightings are clustered into geographic regions using the KMeans algorithm. Each sighting is assigned a `region_id` based on its coordinates. By default, the number of clusters is set to 10. This regional grouping provides the model with another categorical feature that should capture local patterns in wolf activity. After fitting the model, we save it to a file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4040ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "processed_df['region_id'] = kmeans_model.fit_predict(processed_df[['lat', 'lon']].values)\n",
    "\n",
    "# Save the model\n",
    "with open(kmeans_path, 'wb') as f:\n",
    "    pickle.dump(kmeans_model, f)\n",
    "\n",
    "print(\"KMeans model saved to disk.\")\n",
    "print(\"Processed DataFrame with region IDs:\")\n",
    "print(processed_df[['lat', 'lon', 'region_id']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7a697",
   "metadata": {},
   "source": [
    "As the spatial processing of data concludes, we visualize the sightings on an interactive map. Colors are assigned based on region IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c13bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_center = [processed_df['lat'].mean(), processed_df['lon'].mean()]\n",
    "\n",
    "preview_map = folium.Map(location=map_center, zoom_start=9)\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'purple', 'yellow', 'orange', 'darkred', 'lightblue', 'darkgreen', 'cadetblue', 'gray']\n",
    "\n",
    "for _, row in processed_df.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        tooltip=f\"{row['ort']} (Cluster {row['region_id']})\",\n",
    "        color=colors[int(row['region_id']) % len(colors)],\n",
    "        fill=True,\n",
    "        fill_opacity=0.6,\n",
    "        radius=3,\n",
    "    ).add_to(preview_map)\n",
    "    \n",
    "preview_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f526ac7",
   "metadata": {},
   "source": [
    "### Temporal and Historic Feature Engineering\n",
    "\n",
    "Next, we enrich the sightings data with time-based features. The date column is converted to a proper timestamp, and entries with invalid dates are removed. From the timestamp, two new features are extracted: The month, which is numerical, and the season, which is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0365284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'datum' column to datetime\n",
    "processed_df['timestamp'] = processed_df['datum'].apply(date_string_to_datetime)\n",
    "\n",
    "# Remove entries with malformed dates\n",
    "processed_df = processed_df[processed_df['timestamp'].notnull()]\n",
    "\n",
    "processed_df['month'] = processed_df['timestamp'].dt.month\n",
    "processed_df['season'] = processed_df['month'].apply(season_from_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a6ef1",
   "metadata": {},
   "source": [
    "#### Recent Sightings in the Region\n",
    "\n",
    "Next, we introduce a feature that counts how many wolf sightings occurred in the same region within the last 30 days before each individual event. This gives the model a sense of recent wolf activity in the area, allowing it to factor in short-term trends and patterns. The feature is stored in a new column called `recent_sightings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29040353",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['recent_sightings'] = processed_df.apply(\n",
    "    lambda row: count_sightings_in_region(\n",
    "        processed_df,\n",
    "        row['region_id'],\n",
    "        row['timestamp'] - recent_duration,\n",
    "        row['timestamp']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed79ac4",
   "metadata": {},
   "source": [
    "#### Last Sighting in the Region\n",
    "\n",
    "Another temporal feature is added, this time counting the number of days since the last wolf sighting in the same region. If no earlier sightings exist, a high default value is used. This helps the model capture longer-term patterns of wolf presence or absence in an area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59249608",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['days_since_last_sighting'] = processed_df.apply(\n",
    "    lambda row: count_days_since_last_sighting(\n",
    "        processed_df, row['region_id'], row['timestamp']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923c9a5",
   "metadata": {},
   "source": [
    "This concludes the preprocessing of the data. We can save the processed data, including the geocoded coordinates, region IDs, and temporal features, to a CSV file in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0440d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveDataFrameWidget(\n",
    "    df=processed_df,\n",
    "    path=preprocessed_data_path,\n",
    "    label=\"Save Processed Data\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcf430",
   "metadata": {},
   "source": [
    "## Event Generation\n",
    "\n",
    "After preparing the sightings data through spatial and temporal enrichment, the next stage involves constructing a balanced dataset of events. Events are instances in space and time, labeled either as sightings or as synthetic non-sightings. The resulting event dataset enables the formulation of wolf sighting prediction as a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b4ef1",
   "metadata": {},
   "source": [
    "The below widget is provided for loading the previously generated sighting data and the KMeans model from the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52defe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_processed_df(df):\n",
    "    global processed_df\n",
    "    processed_df = df\n",
    "\n",
    "def set_kmeans_model(model):\n",
    "    global kmeans_model\n",
    "    kmeans_model = model\n",
    "\n",
    "LoadDataWidget(\n",
    "    path=preprocessed_data_path,\n",
    "    on_load=set_processed_df,\n",
    "    label=\"Load Processed Data\"\n",
    ").display()\n",
    "LoadDataWidget(\n",
    "    path=kmeans_path,\n",
    "    on_load=set_kmeans_model,\n",
    "    label=\"Load KMeans Model\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed1d57",
   "metadata": {},
   "source": [
    "A new empty DataFrame is created to hold both positive (actual wolf sightings) and later negative (no sighting) events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f950c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sighting_events = pd.DataFrame(columns=[\n",
    "    'is_sighting',\n",
    "    'month',\n",
    "    'season',\n",
    "    'region_id',\n",
    "    'coord_bin',\n",
    "    'lat', 'lon',\n",
    "    'recent_sightings',\n",
    "    'days_since_last_sighting',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97647f7",
   "metadata": {},
   "source": [
    "All real wolf sightings from the processed data are added to the `sighting_events` table as positive examples, labeled with `is_sighting = 1`. Each record carries the spatial, temporal, and historical features that were generated during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cd16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in processed_df.iterrows():\n",
    "    sighting_events = pd.concat([sighting_events, pd.DataFrame([{\n",
    "        'is_sighting': 1,\n",
    "        'month': row['month'],\n",
    "        'season': row['season'],\n",
    "        'region_id': row['region_id'],\n",
    "        'coord_bin': row['coord_bin'],\n",
    "        'lat': row['lat'], 'lon': row['lon'],\n",
    "        'recent_sightings': row['recent_sightings'],\n",
    "        'days_since_last_sighting': row['days_since_last_sighting']\n",
    "    }])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47226cd5",
   "metadata": {},
   "source": [
    "Next, we create synthetic negative events — moments and locations where no wolf sightings were recorded. For each year in the dataset, an equal number of negative events are generated by randomly selecting times and locations within the overall bounds of the sightings data. Each synthetic event is labeled with `is_sighting = 0`, and the same spatial, temporal, and historical features are calculated as for real sightings. This is intended to balance the dataset and should help the model learn to distinguish true sightings from typical background activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat = processed_df['lat'].min()\n",
    "max_lat = processed_df['lat'].max()\n",
    "min_lon = processed_df['lon'].min()\n",
    "max_lon = processed_df['lon'].max()\n",
    "\n",
    "# Ensure the 'timestamp' column is of datetime type\n",
    "processed_df['timestamp'] = pd.to_datetime(processed_df['timestamp'])\n",
    "\n",
    "def random_timestamp(year):\n",
    "    start = datetime(year, 1, 1)\n",
    "    end = datetime(year + 1, 1, 1) - timedelta(seconds=1)\n",
    "    delta = end - start\n",
    "    random_seconds = random.randint(0, int(delta.total_seconds()))\n",
    "    return start + timedelta(seconds=random_seconds)\n",
    "\n",
    "for year in processed_df['jahr'].unique():\n",
    "    # Get the number of real data points for the year\n",
    "    num_events = processed_df[processed_df['jahr'] == year].shape[0]\n",
    "    # Generate negative events\n",
    "    for _ in range(num_events):\n",
    "        # Generate a random timestamp within the year\n",
    "        timestamp = random_timestamp(int(year))\n",
    "        month = timestamp.month\n",
    "        season = season_from_month(month)\n",
    "\n",
    "        # Generate random coordinates within the bounding box\n",
    "        lat = random.uniform(min_lat, max_lat)\n",
    "        lon = random.uniform(min_lon, max_lon)\n",
    "        # Determine the region_id based on the coordinates\n",
    "        region_id = kmeans_model.predict([[lat, lon]])[0]\n",
    "\n",
    "        # Determine the recent sightings and days since last sighting\n",
    "        recent_sightings = count_sightings_in_region(processed_df, region_id, timestamp - recent_duration, timestamp)\n",
    "        days_since_last_sighting = count_days_since_last_sighting(processed_df, region_id, timestamp)\n",
    "\n",
    "        # Append the negative event\n",
    "        sighting_events = pd.concat([sighting_events, pd.DataFrame([{\n",
    "            'is_sighting': 0,\n",
    "            'month': month,\n",
    "            'season': season,\n",
    "            'region_id': region_id,\n",
    "            'coord_bin': coordinate_bin(lat, lon),\n",
    "            'lat': lat, 'lon': lon,\n",
    "            'recent_sightings': recent_sightings,\n",
    "            'days_since_last_sighting': days_since_last_sighting\n",
    "        }])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c0afe",
   "metadata": {},
   "source": [
    "Finally, all sighting events are shuffled. This way, a test/train split can be obtained without requiring random sampling, which allows for easy subset creation in DBRepo. The final dataset is saved to the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d31402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the events\n",
    "sighting_events = sighting_events.sample(frac=1).reset_index(drop=True)\n",
    "# Generate an ID for each event\n",
    "sighting_events['id'] = range(1, len(sighting_events) + 1)\n",
    "\n",
    "# Save the sighting events to a CSV file\n",
    "sighting_events.to_csv(events_path, index=False)\n",
    "print(f\"Sighting events saved to {events_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a209e",
   "metadata": {},
   "source": [
    "## Upload Data to Repository\n",
    "\n",
    "The generated KMeans model can be uploaded to [TU Wien Research Data](https://test.researchdata.tuwien.ac.at/records/j49xf-khk42) using the widget below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = TuwrdUploadWidget(base_url=\"https://test.researchdata.tuwien.at\", record_id='j49xf-khk42', label=\"Upload KMeans Model\")\n",
    "widget.add_file(\"data/wolf_sightings_kmeans.pkl\", \"wolf_sightings_kmeans.pkl\")\n",
    "widget.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
